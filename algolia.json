[{"content":" Human\u0026amp;rsquo;s desire for efficiency led to the development of computers and programming languages. Enabling computers to do tasks that can be done by humans and also the tasks that can\u0026amp;rsquo;t be done by humans and thus humans can work less and spend more time on other things. Machines can work 24 X 7 unlike humans who can only work for a few hours a day. From data mining to knowledge discovery in databases (1996 paper) Data mining is the application of specific algorithms for extracting patterns from databases Data mining + Computer Science = Data Science Business Intelligence: find if somebody is pregnant from their purchases data and market to them the things that you sell to new parents Paper title: What\u0026amp;rsquo;s even creepier than target guessing that you are pregnant In the move \u0026amp;ldquo;Money ball\u0026amp;rdquo;, a poor baseball team picked under valued players and win 20 consecutive series using data Science  Data Science Project Steps  Formulate Question: Define the problem that you want to solve and make sure you ask the right questions; clear well formulated question will determine the research and it will also affect the kind of data that you will gather Gather data:  Source of the data: Where does the data come From Description of the data set: understanding all the context under which the data was collected Number of data points: How big is the dataset actually Number of features: for each data point, how many aspects (characteristics) were measured Description of the features:   Clean data: real world data is also messy, so need to clean the data Explore and Visualize: a graph or chart is much more helpful than a table of databases; a picture is worth a thousand words; Oftentimes, exploring, visualizing and cleaning the data more or less at the same time  Distribution Outliers   Model: Train Algorithm  Split training data and test data: shuffle data before splitting   Deploy and Evaluate  Check-up list to evaluate regression model:  R-squared p-values : of the …","date":-62135596800,"description":"Text about this post","objectID":"3976528693a0108357f4928017600865","permalink":"https://bharathravu.github.io/data-science-notes/","title":"Home title"},{"content":"Regression is fitting a line to our data; Discovering the hidden relationship between input and output.\nUnivariate linear regression  Linear regression with one independent variable (or feature). a training example: $(x^{(i)},y^{(i)})$, where $i=1,\\ldots,m$. $m$ represents the number of training examples. $X$ - space of input variables; $Y$ - space of output variables $h: X \\mapsto Y$, where $h$ is a hypothesis. Hypothesis: $h_{\\theta} (x)=\\theta_0 + \\theta_1 x $; Shorthand notation: $h(x)$. $\\theta_0$ is intercept, $\\theta_1$ is slope. $\\theta_{i\u0026amp;rsquo;s}:$ parameters; How to choose $\\theta_{i\u0026amp;rsquo;s}$? Choose $\\theta_0$, $\\theta_1$ so that $h(x)$ is close to $y$ for the training set. Cost function: Squared error function \\begin{equation} J(\\theta_0, \\theta_1)= \\frac{1}{2m} \\sum_{i=1}^m (h_{\\theta}(x^{(i)})-y^{(i)})^2 \\label{eq:cost_fn_linear_uni} \\end{equation} \\begin{equation*} J(\\theta_0, \\theta_1)= \\frac{1}{2m} \\sum_{i=1}^m (\\theta_0 + \\theta_1 x^{(i)}-y^{(i)})^2 \\end{equation*} Goal: $\\underset{\\theta_0, \\theta_1}{\\text{Minimize}}$ $J$; At $J$ minimum, $\\frac{\\partial J}{\\partial \\theta_0}=0$ and $\\frac{\\partial J}{\\partial \\theta_1}=0$. Sometimes unrealistic values of $\\theta_0$ would be found one unit change in X gives $\\theta_1$ times change in y Goodness of fit, $R^2$= the amount of variance explained by the model, range 0-1; if $R^2$=0.55, 55% variance is explained by the model. $R^2$: 0 (bad) $\\to$ $1 (good)$  Derivation of coefficients Expanding equation \\eqref{eq:cost_fn_linear_uni}, \\begin{equation} J(\\theta_0, \\theta_1)= \\frac{1}{2m} \\sum_{i=1}^m \\Big(\\theta_0^2+\\theta_1^2 (x^{(i)})^2 +(y^{(i)})^2 + 2 \\theta_0 \\theta_1 x^{(i)} - 2 \\theta_1 x^{(i)} y^{(i)} - 2 \\theta_0 y^{(i)}\\Big)\n\\end{equation} Calculating $\\frac{\\partial J}{\\partial \\theta_0}$, \\begin{equation} \\begin{split} \\frac{\\partial J}{\\partial \\theta_0} \u0026amp;amp; = \\frac{1}{2m} \\sum_{i=1}^m (2 \\theta_0 + 2 \\theta_1 x^{(i)}- 2 y^{(i)}) \\cr \u0026amp;amp; = \\theta_0 + \\theta_1 \\frac{ \\sum_{i=1}^m …","date":-62135596800,"description":"","objectID":"bd313636756ff176ff16500f85ac2d27","permalink":"https://bharathravu.github.io/data-science-notes/chapter1-machilelearning/1_linear_regression/","title":"Linear Regression"},{"content":"Coursera Notes:\nClassification  Email: spam / not spam Online transactions: Fraudulent (yes/no) Tumor: Malignant/Benign ?  $y=0$: negative Class (Benign tumor) $y=1$: positive class (Malignant)   Threshold classifier output $h_{\\theta}$ at 0.5:  if $h_{\\theta} \\geq 0.5$, predict $y=1$ if $h_{\\theta} \u0026amp;lt; 0.5$, predict $y=0$   Classification $y=0$ or $y=1$. But if we use linear regression formula, $h_{\\theta}(x)$ can be \u0026amp;lt;0 or \u0026amp;gt;1 Logistic regression model: want $0 \\leq h_{\\theta} \\leq 1$ Sigmoid function $g(z)$: output values range from 0 to 1 \\begin{equation} g(z)=\\frac{1}{1+\\exp^{-z}} \\end{equation} set, $z=\\Theta^T x$ Hypothesis: \\begin{equation} h_{\\theta} (x) = \\frac{1}{1+\\exp^{-\\Theta^T x}} \\end{equation}  Interpretation of hypothesis output  $h_{\\theta}(x)$ is the estimated probability that $y=1$ on input $x$ $h_{\\theta}(x)=0.7$, tell the patient that 70 % chance of tumor being malignant. $h_{\\theta}=P(y=1|x; \\Theta)$, probability that $y=1$, given $x$ parameterized by $\\Theta$ $h_{\\theta}=P(y=0|x; \\Theta) + h_{\\theta}=P(y=1|x; \\Theta) =1$\n$h_{\\theta}=P(y=0|x; \\Theta) = 1 - h_{\\theta}=P(y=1|x; \\Theta) $  Decision boundary  Decision boundary depends on the threshold that has been set at $h_{\\theta}(x)=g(\\Theta^T x)= 0.5$ $h_{\\theta}(x)= 0.5$, when $z=\\Theta^T x=0$ so the line $\\Theta^T x=0$ in the space $X$ becomes the decision boundary  predict $y=1$ if $\\Theta^T x \\geq 0$ predict $y=0$ if $\\Theta^T x \u0026amp;lt; 0$    Non-linear decision boundary  Threshold at $h_{\\theta}(x)=g(\\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 x_1^2 + \\theta_4 x_2^2)=0.5$, where $g$ is the sigmoid function  decision boundary $\\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 x_1^2 + \\theta_4 x_2^2=0$   Imagine a decision boundary which is a unit circle centered at origin $(x_1=0,x_2=0)$, how would the hypothesis look like?  $h_{\\theta}(x)=g(x_1^2+x_2^2-1)$    Cost function  Can\u0026amp;rsquo;t use $J= \\frac{1}{2m} \\sum_{i=1}^m \\Big( h_{\\theta}(x^{(i)})-y^{(i)} \\Big)^2$, this makes the …","date":-62135596800,"description":"","objectID":"11555c508d5b3dfdbb825bb5ec2382f0","permalink":"https://bharathravu.github.io/data-science-notes/chapter1-machilelearning/2_logistic_regression/","title":"Logistic Regression"},{"content":" Human brain has approximately 86 billion neuros which are connected to each other via synapses. Neurons receive signals from other neurons via their dendrites and transmit their output through their axons. A neuron only fires if the total signal received exceeds a certain threshold value. \u0026amp;ldquo;Neural pathways are strengthened every time that they are used\u0026amp;rdquo; -Donald hebb. If two neurons fire together, their connection is enhanced. The strength is referred to as the weight. We are effectively training our neurons and changing the weights between them. The learning comes down to adjusting the weights between the neurons and that is also the model that artificial neural network based on. With deep learning, neural network will do the job of feature selection. So deep learning removes the whole feature selection process. Activation function: It determines whether a neuron would activate or not. Examples are sigmoid, tanh or relu functions. Each node in the input layer represents a feature. The goal of a neural network is to discover the optimal combination of features. Since all neurons are connected together, it gets to try out every single combination. If pixels of an image is given as an input, the first hidden layer will start to generate features (or detecting simple patterns) like lines, edges, textures. The second hidden layer would use the features that the first layer outputs it. Here, the second layer is no longer confronted with pixels but with features generated by the first layer. Again the second hidden layer will try to combine lines and edges into something like shapes like rectangles, circles or shadows or something. Again the third hidden layer would get these shapes and generates its own features like eyes or tail or legs. This process continues till the final layer. In general, we don\u0026amp;rsquo;t know exactly what features the neural network generates. Rule of 10: we probably need 10 times more the amount of data as we have parameters that we need …","date":-62135596800,"description":"","objectID":"1c080eab5996813ef2b2304bc3fd7acb","permalink":"https://bharathravu.github.io/data-science-notes/chapter1-machilelearning/3_ann/","title":"Artificial neural networks"},{"content":" LSTM is awesome, but not good enough Transformers: how and why $f(X) \\approx y$, where $f$ is the model, X is the input and $y$ is the outcome/prediction Sequence Modeling is a problem $f:\\mathbb{R}^d \\mapsto \\mathbb{R}$, $\\mathbb{R}^d$ contains fixed size $d$ vectors can not represent documents a fixed size vector Documents are of various lengths Not aware of any linear algebra that works on variable dimensionality Classic way: bag of words Order matters: \u0026amp;ldquo;work to live\u0026amp;rdquo; vs \u0026amp;ldquo;live to work\u0026amp;rdquo;; Both score same. Solution would be N-grams dimensionality $V^N$ Bi-grams (Every pair of possible words) Tri-grams (Every combination of three words) This way we can distinguish between the two English tri-grams: (10^15) dimensions All sort of problems with that size of dimensions. A natural way to solve this problem is the RNN (Recurrent Neural Network) How to calculate $f(x_1,x_2,x_3,\\cdots,x_n)$ A for-loop in math $H_{i+1}=A(H_i,X_I$) The problem with RNN is vanishing and exploding gradients\n$H_3= A(A(A(H_0,x_0),x_1),x_2)$\n$A(H,x):=Wx+ZH$\n$H_N=W^N x_0 + W^{N-1} x_1 + \u0026amp;hellip;$ For 100 words, W^100, $0.9^100=7 \\times 10^{-10}$, $1.1^100=189905276$ In linear algebra, this is about same except we need to think about eigenvalues of the matrix Eigenvalues say- how much the matrix is going to grow or shrink vectors when the transformations are applied If the eigenvalues are les than one, we see the gradients go to zero. If #\\lambda\u0026amp;gt;1$, gradients are going to explode. This made RNN extremely difficult, So LSTM to the rescue LSTM is a kind of Learning; Here it is not applied recursively on the main hidden vector, it is not like a CNN resnet.  ","date":-62135596800,"description":"","objectID":"b57b7ca0cd89ac64d2685e9343a30111","permalink":"https://bharathravu.github.io/data-science-notes/chapter1-machilelearning/6_sequence_modeling/","title":"Sequence Modeling"},{"content":"$ \\mathbf{Independent variables}$ (Features or inputs or $x$): These don\u0026amp;rsquo;t depend on anything and also are inputs to the model $f$. Here $x$ is a vector.\n$ \\mathbf{Dependent variable}$ (Target or $y$): The value of this variable depends on independent variables\n\\begin{equation} f(x) \\approx y, \\end{equation} where $f$ is a function that we want to find out using $x$ and $y$.\n$ \\mathbf{Supervised Learning:}$ Using known values of $x$ and the corresponding $y$ values, we want to construct $f$. We, then use $f$ and try to predict $y$. There are two categories.\n Regression - for continuous target values Classification - for discrete target values\n$ \\mathbf{Unsupervised Learning:}$ We want to desire structure from data that we have no idea about how our target should be. Examples: clustering (stocks of various companies), non-clustering (Cocktail Party Algorithm)  ","date":-62135596800,"description":"","objectID":"d063c8d0a76472b1fb6422e2fa3fea92","permalink":"https://bharathravu.github.io/data-science-notes/chapter1-machilelearning/","title":"Machine Learning"},{"content":" We should look at the correlation of our features with our target but also at the correlation between our different features. Correlation is the degree to which things move together The amount of sun and the amount of icecream tend to move together; if one is high, the other also tends to be high and if one is low the other also tends to be low. This is positive correlation. Negative correlation: If one is high, other tends to be lower No correlation: Imagine how a graph would look unlike Correlation is calculated as a single number which ranges between -1 to 1\n$\\rho_{XY}=corr(X,Y)$ $\\rho=0$ means no correlation or uncorrelated Correlation is a statistical measure of a linear relationship between two variables Why should we look at the correlations of our features during the data exploration stage. The answer is that we primarily care about two things  Strength: Strength of the correlation; it is important because it tells us how much correlation is there Direction: direction of the correlation   Our model should include features that are correlated with the target; want to include features whose movement is associated with a big movement of target value; want a correlation that is not close to zero Python command to calculate all the correlations between all the features; correlation matrix  data.corr()  There are multiple ways to calculate a correlation and the default way of doing this calculation is the Pearson correlation; assumption: It is only valid for continuous variables. That means it is valid for dummy variables like whether a property is on cahrles river or not. If two features were highly correlated would that be a good thing or a bad thing for our regression Modeling and the answer is it depends High correlation between features can be problematic; so this we probably want to discover early on Example: Predict bone density using: Age, bodyfat, weight Because bodyfat and weight move together you are going to have difficulty telling apart their effects …","date":-62135596800,"description":"","objectID":"0044c93a4bfa50e7f8166aaecf4f7d6a","permalink":"https://bharathravu.github.io/data-science-notes/chapter2-statistical-methods/1/","title":"Correlation"},{"content":"","date":-62135596800,"description":"","objectID":"a06ef128fd5fb643cce92b0e744e1c92","permalink":"https://bharathravu.github.io/data-science-notes/chapter2-statistical-methods/","title":"Statistics"}]